{
  "hash": "39fd8ed5c56f1c8ba215f52cc7714116",
  "result": {
    "markdown": "---\ntitle: \"Time Series Project\"\nauthor: \"Karol Orozco\"\ndate: \"2022-12-04\"\n---\n\n\n## Background\n\nIn this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\n\nHere is the link: https://www.zillow.com/research/data/\n\n\n\n\n\n## The Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrofour <- read.csv(\"C:/Users/karol/Desktop/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13513 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ X2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ X2000.03.31: num  289205 304587 146194 182517 154357 ...\n```\n:::\n:::\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\n\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,…)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3702562     Length:3702562    \n 1st Qu.: 17364   1st Qu.: 3506   Class :character   Class :character  \n Median : 31949   Median : 7193   Mode  :character   Mode  :character  \n Mean   : 51588   Mean   : 8231                                        \n 3rd Qu.: 46308   3rd Qu.:11702                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3702562     Length:3702562     Length:3702562     Length:3702562    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3702562     Min.   :  18032   Min.   :2000-01-31  \n Class :character   1st Qu.: 172756   1st Qu.:2005-09-30  \n Mode  :character   Median : 244796   Median :2011-06-15  \n                    Mean   : 321468   Mean   :2011-06-15  \n                    3rd Qu.: 368862   3rd Qu.:2017-02-28  \n                    Max.   :8241271   Max.   :2022-10-31  \n                    NA's   :1195958                       \n```\n:::\n:::\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:106         Length:106        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:106         Length:106         Length:106         Length:106        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:106         Min.   :411049   Min.   :2014-01-31  \n Class :character   1st Qu.:505802   1st Qu.:2016-04-07  \n Mode  :character   Median :564917   Median :2018-06-15  \n                    Mean   :562387   Mean   :2018-06-15  \n                    3rd Qu.:584219   3rd Qu.:2020-08-23  \n                    Max.   :759661   Max.   :2022-10-31  \n```\n:::\n:::\n\nAfter filtering the data, we don’t have any missing values\n\n### Coerce to a tsibble with as_tsibble()\n\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)\n```\n:::\n\n\n## Data Visualization\n\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2012-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n```{.r .cell-code}\nplot_pdx_house \n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Determining Stationarity\n\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.95        0.01\n```\n:::\n:::\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n```\n:::\n:::\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary.\n\n\n## Autocorrelation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\n\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\n\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nThese are also clearly non-stationary, so we take a further first difference\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12) %>% difference(),\n               plot_type='partial', lag=36) +\n  labs(title = \"Double differenced\", y=\"\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph.\n\n\n## Seasonal Arima Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.86   -189.  391.  392.  409.\n2 search        2.17   -190.  396.  398.  418.\n3 stepwise      2.25   -193.  399.  400.  418.\n4 arima210011   4.58   -205.  419.  419.  429.\n```\n:::\n:::\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\n::: {.cell}\n\n```{.r .cell-code}\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5148  0.0355  1.0100  0.9999  -0.8229  0.1287\ns.e.  0.1145  0.1214  0.0582  0.0709   0.1477  0.1514\n\nsigma^2 estimated as 2.856:  log likelihood=-188.55\nAIC=391.09   AICc=392.41   BIC=408.82\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    37.3     0.170\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model\\napplied to the Portland House Prices data\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_m…¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Nov   N(737, 3)  737.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Dec  N(737, 22)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jan  N(739, 76)  739.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Feb N(742, 157)  742.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Mar N(748, 257)  748.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Apr N(753, 369)  753.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 May N(758, 487)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jun N(761, 609)  761.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jul N(764, 734)  764.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Aug N(765, 860)  765.\n# … with 14 more rows, and abbreviated variable name ¹​year_month\n```\n:::\n:::\n\n\n## ETS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9987271 \n    phi   = 0.9108026 \n\n  Initial states:\n     l[0]     b[0]\n 407.9964 3.526883\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n650.8419 651.6904 666.8225 \n```\n:::\n:::\n\n\nThe model selected is ETS(M,Ad,N)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPlot variable not specified, automatically selected `.vars = value`\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets%>%\n    gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`\n```\n:::\n\n::: {.cell-output-display}\n![](TimeSeries_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.53  1.14 0.194 0.0286 0.0310\n2 Portland   ETS(Prices) Training  2.27  1.60 0.271 0.0403 0.0459\n```\n:::\n:::\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE.\n",
    "supporting": [
      "TimeSeries_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}