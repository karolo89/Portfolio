---
title: "House Prices in Porltand, OR"
author: "Karol Orozco"
date: "2022-12"
---

## Background

the goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area.

```{r}
library(tidyverse)
library(ggplot2)
library(readr)
library(tidymodels)

```

## Get the Data

```{r}
raw_pdx <- read.csv("C:/Users/karol/Desktop/PORTLAND HOUSE.csv", stringsAsFactors=TRUE)
```

## Prepare the Data

This data has 25731 obs. of 32 variables

```{r}
## raw_pdx <- raw_pdx%>%select(-id)

head(raw_pdx)
```

```{r}

# convert variables

raw_pdx <-  raw_pdx %>% 
  
  mutate(
    
    yearBuilt = as.numeric(yearBuilt),
    bathrooms = as.numeric(bathrooms),
    bedrooms = as.numeric(bedrooms),
    daysOnZillow = as.numeric(daysOnZillow),
    lastSoldPrice = as.numeric(lastSoldPrice ),
    livingArea = as.numeric(livingArea),
    lotSize= as.numeric(lotSize),
    price = as.numeric(price),
    priceHistory.1.price= as.numeric(priceHistory.1.price),
    
    
    schools0rating = as.factor(schools0rating),
    schools1rating = as.factor(schools1rating),
    schools2rating= as.factor(schools2rating),
    zipcode = as.factor(zipcode)

    
  )

```

### Missing data

```{r}
is.na(raw_pdx) %>% colSums()
```

```{r}
clean_data <- raw_pdx %>%
  filter(!is.na(yearBuilt))%>%
  filter(!is.na(longitude))%>%
  filter(!is.na(bedrooms))%>%
  filter(!is.na(daysOnZillow))%>%
  filter(!is.na(livingArea))%>%
  filter(!is.na(priceHistory.1.price))%>%
  filter(!is.na(hasFireplace))%>%
  filter(!is.na(latitude))%>%
  filter(!is.na(hasHeating))%>%
  filter(!is.na(hasCooling))%>%
  filter(!is.na(bathrooms))%>%
  filter(!is.na(lotSize))%>%
  filter(!is.na(propertyTaxRate))%>%
  filter(!is.na(schools0distance))%>%
  filter(!is.na(schools1distance))%>%
  filter(!is.na(schools2distance))%>%
  filter(!is.na(schools0rating))%>%
  filter(!is.na(schools2rating))%>%
  filter(!is.na(schools1rating))%>%
  filter(!is.na(schools1rating))

```

```{r}
summary(clean_data)

```

```{r}
clean_data <- 
  clean_data %>% 
  mutate(price_category = case_when( 
    price < 551000 ~ "below",
    price >= 551000 ~ "above")) %>% 
  mutate(price_category = as.factor(price_category)) 
```

```{r}
library(gt) ## tables

clean_data %>% 
  count(price_category, 
        name ="total") %>%
  mutate(percent = total/sum(total)*100,
         percent = round(percent, 2)) %>%
 gt() %>%
  tab_header(
    title = "Portland, OR and its Metropolitan Area Median House Prices",
    subtitle = "Above and below 551,000$"
  ) %>%
  cols_label(
    price_category = "Price",
    total = "Total",
    percent = "Percent"
  ) %>% 
  fmt_number(
    columns = vars(total),
    suffixing = TRUE
  ) 
```

```{r}

library(ggmap)

qmplot(x = longitude, 
       y = latitude, 
       data = clean_data, 
       geom = "point", 
       color = price_category, 
       alpha = 0.4) +
  scale_alpha(guide = 'none')
```

This image tells you that the housing prices are related to the location.

```{r}
houses_pdx <-
  clean_data %>% 
  select( # select our predictors
    longitude, 
    latitude, 
    price_category, 
    bathrooms, 
    yearBuilt, 
    homeType,
    bedrooms, 
    livingArea, 
    lotSize,
    schools2distance,
    schools1distance,
    schools0distance,
    schools1distance)

glimpse(houses_pdx)

```

### Data Splitting

```{r}

# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(504)

# Put 3/4 of the data into the training set 
data_split <- initial_split(houses_pdx, 
                           prop = 3/4)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```

### Validaton Set

```{r}
cv_folds <-
 vfold_cv(train_data, 
          v = 5, 
          strata = price_category) 
```

```{r}
pdx_rec <-
  recipe(price_category ~ .,
         data = train_data) %>%
  update_role(longitude, latitude, 
              new_role = "ID") %>% 
  
  step_log(bathrooms, bedrooms) %>% ## step_log() will log transform data
  
  step_naomit(everything(), skip = TRUE) %>% 
  
  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.
  
  step_normalize(all_numeric(), -all_outcomes(), 
                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero
  
  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.
  
  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.
  
  step_corr(all_predictors(), threshold = 0.7, method = "spearman") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.

```

```{r}
summary(pdx_rec)

```

```{r}
prep_data <- 
  pdx_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

```

## The Model- Logistic regression

```{r}
log_spec <- # your model specification
  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec
```

```{r}
log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(pdx_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

log_wflow

```

```{r}
log_res <- log_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
    recall, precision, f_meas, 
    accuracy, kap,
    roc_auc, sens, spec),
    control = control_resamples(
    save_pred = TRUE)
    ) 
```

```{r}
# save model coefficients for a fitted model object from a workflow

get_model <- function(x) {
  pull_workflow_fit(x) %>% tidy()
}

# same as before with one exception
log_res_2 <- 
  log_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(
      save_pred = TRUE,
      extract = get_model) # use extract and our new function
    ) 

log_res_2$.extracts[[1]]

```

To get the results use:

```{r}
log_res_2$.extracts[[1]][[1]]

```

All of the results can be flattened and collected using:

```{r}

all_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])
filter(all_coef, term == "livingArea")

```

### Performance metrics

```{r}
log_res %>%  collect_metrics(summarize = TRUE)

```

Show performance for every single fold:

```{r}
log_res %>%  collect_metrics(summarize = FALSE)

```

### Collect predictions

To obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:

```{r}
log_pred <- 
  log_res %>%
  collect_predictions()

log_pred %>% 
  conf_mat(price_category, .pred_class) 
```

```{r}
log_pred %>% 
  conf_mat(price_category, .pred_class) %>% 
  autoplot(type = "heatmap")
```

### ROC Curve

```{r}
log_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(price_category, .pred_above) %>% 
  autoplot()
```

```{r}
log_pred %>% 
  ggplot() +
  geom_density(aes(x = .pred_above, 
                   fill = price_category), 
               alpha = 0.5)
```
