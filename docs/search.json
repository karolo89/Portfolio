[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Dec 4, 2022\n\n\nKarol Orozco\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nKarol Orozco\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/House Prices in Porltand/Portland Prices- Regression.html",
    "href": "posts/House Prices in Porltand/Portland Prices- Regression.html",
    "title": "House Prices in Porltand, OR",
    "section": "",
    "text": "the goal is to build a classification model to predict the type of median housing prices in Portland, OR and its metropolitan area."
  },
  {
    "objectID": "posts/House Prices in Porltand/Portland Prices- Regression.html#get-the-data",
    "href": "posts/House Prices in Porltand/Portland Prices- Regression.html#get-the-data",
    "title": "House Prices in Porltand, OR",
    "section": "Get the Data",
    "text": "Get the Data\n\n\nShow the code\nclean_data <- read.csv(\"C:/Users/karol/Desktop/clean_data_pdx.csv\", stringsAsFactors=TRUE)"
  },
  {
    "objectID": "posts/House Prices in Porltand/Portland Prices- Regression.html#take-a-look-at-the-data",
    "href": "posts/House Prices in Porltand/Portland Prices- Regression.html#take-a-look-at-the-data",
    "title": "House Prices in Porltand, OR",
    "section": "Take a look at the Data",
    "text": "Take a look at the Data\n\n\nShow the code\nlibrary(gt) ## tables\n\n\nWarning: package 'gt' was built under R version 4.2.2\n\n\nShow the code\nclean_data %>% \n  count(price_category, \n        name =\"total\") %>%\n  mutate(percent = total/sum(total)*100,\n         percent = round(percent, 2)) %>%\n gt() %>%\n  tab_header(\n    title = \"Portland, OR and its Metropolitan Area Median House Prices\",\n    subtitle = \"Above and below 551,000$\"\n  ) %>%\n  cols_label(\n    price_category = \"Price\",\n    total = \"Total\",\n    percent = \"Percent\"\n  ) %>% \n  fmt_number(\n    columns = vars(total),\n    suffixing = TRUE\n  ) \n\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      Portland, OR and its Metropolitan Area Median House Prices\n    \n    \n      Above and below 551,000$\n    \n  \n  \n    \n      Price\n      Total\n      Percent\n    \n  \n  \n    above\n7.25K\n50.02\n    below\n7.24K\n49.98\n  \n  \n  \n\n\n\n\n\n\nShow the code\nlibrary(ggmap)\n\n\nWarning: package 'ggmap' was built under R version 4.2.2\n\n\nℹ Google's Terms of Service: <https://mapsplatform.google.com>\n\n\nℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n\n\nShow the code\nqmplot(x = longitude, \n       y = latitude, \n       data = clean_data, \n       geom = \"point\", \n       color = price_category, \n       alpha = 0.4) +\n  scale_alpha(guide = 'none')\n\n\nℹ Using `` zoom = `10` ``\n\n\nℹ Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\n\n\n\n\n\nThis image tells you that the housing prices are related to the location.\n\n\nShow the code\nhouses_pdx <-\n  clean_data %>% \n  select( # select our predictors\n    longitude, \n    latitude, \n    price_category, \n    bathrooms, \n    yearBuilt, \n    homeType,\n    bedrooms, \n    livingArea, \n    lotSize,\n    schools2distance,\n    schools1distance,\n    schools0distance,\n    schools1distance)\n\nglimpse(houses_pdx)\n\n\nRows: 14,488\nColumns: 12\n$ longitude        <dbl> -122.4418, -122.4532, -122.4444, -122.4162, -122.4513…\n$ latitude         <dbl> 45.54357, 45.54758, 45.48823, 45.48799, 45.49818, 45.…\n$ price_category   <fct> below, below, below, below, below, below, below, belo…\n$ bathrooms        <dbl> 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 3.0…\n$ yearBuilt        <int> 2007, 2001, 1982, 1967, 1978, 2018, 2006, 2017, 1958,…\n$ homeType         <fct> TOWNHOUSE, SINGLE_FAMILY, SINGLE_FAMILY, SINGLE_FAMIL…\n$ bedrooms         <int> 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 2, 4, 3, 4, 3, 3, 4,…\n$ livingArea       <int> 1806, 1518, 2724, 1150, 2036, 1947, 1548, 2209, 2838,…\n$ lotSize          <int> 1555, 3484, 9583, 7000, 6969, 4791, 5009, 5227, 6480,…\n$ schools2distance <dbl> 2.6, 3.4, 1.4, 1.4, 1.4, 2.2, 1.5, 1.4, 0.9, 2.3, 2.2…\n$ schools1distance <dbl> 1.1, 1.0, 1.7, 0.4, 2.1, 2.5, 0.5, 1.8, 0.3, 2.6, 2.5…\n$ schools0distance <dbl> 0.4, 1.2, 0.8, 0.4, 1.0, 0.3, 0.5, 1.0, 0.1, 0.4, 0.6…\n\n\n\nData Splitting\n\n\nShow the code\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(504)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(houses_pdx, \n                           prop = 3/4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)\n\n\n\n\nValidaton Set\n\n\nShow the code\nhouse_folds <-\n vfold_cv(train_data, \n          v = 5, \n          strata = price_category) \n\n\n\n\nShow the code\nnames(train_data)\n\n\n [1] \"longitude\"        \"latitude\"         \"price_category\"   \"bathrooms\"       \n [5] \"yearBuilt\"        \"homeType\"         \"bedrooms\"         \"livingArea\"      \n [9] \"lotSize\"          \"schools2distance\" \"schools1distance\" \"schools0distance\"\n\n\n\n\nShow the code\npdx_rec <-\n  recipe(price_category ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  \n  step_log(bathrooms) %>% ## step_log() will log transform data\n  \n  step_naomit(everything(), skip = TRUE) %>% \n  \n  step_novel(all_nominal(), -all_outcomes()) %>% # converts all nominal variables to factors and takes care of other issues related to categorical variables.\n  \n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% # step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% #converts our factor columns into numeric binary (0 and 1) variables.\n  \n  step_zv(all_numeric(), -all_outcomes()) %>% ## step_zv(): removes any numeric variables that have zero variance.\n  \n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") # step_corr(): will remove predictor variables that have large correlations with other predictor variables.\n\n\n\n\nShow the code\nsummary(pdx_rec)\n\n\n# A tibble: 12 × 4\n   variable         type    role      source  \n   <chr>            <chr>   <chr>     <chr>   \n 1 longitude        numeric ID        original\n 2 latitude         numeric ID        original\n 3 bathrooms        numeric predictor original\n 4 yearBuilt        numeric predictor original\n 5 homeType         nominal predictor original\n 6 bedrooms         numeric predictor original\n 7 livingArea       numeric predictor original\n 8 lotSize          numeric predictor original\n 9 schools2distance numeric predictor original\n10 schools1distance numeric predictor original\n11 schools0distance numeric predictor original\n12 price_category   nominal outcome   original\n\n\n\n\nShow the code\nprep_data <- \n  pdx_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe"
  },
  {
    "objectID": "posts/House Prices in Porltand/Portland Prices- Regression.html#the-model--logistic-regression",
    "href": "posts/House Prices in Porltand/Portland Prices- Regression.html#the-model--logistic-regression",
    "title": "House Prices in Porltand, OR",
    "section": "The Model- Logistic regression",
    "text": "The Model- Logistic regression\n\n\nShow the code\nlog_spec <- # your model specification\n  logistic_reg() %>%  # model type\n  set_engine(engine = \"glm\") %>%  # model engine\n  set_mode(\"classification\") # model mode\n\n# Show your model specification\nlog_spec\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\nlog_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(pdx_rec) %>%   # use the new recipe\n add_model(log_spec)   # add your model spec\n\nlog_wflow\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_log()\n• step_naomit()\n• step_novel()\n• step_normalize()\n• step_dummy()\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nShow the code\nlog_res <- log_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n    recall, precision, f_meas, \n    accuracy, kap,\n    roc_auc, sens, spec),\n    control = control_resamples(\n    save_pred = TRUE)\n    ) \n\n\n! Fold5: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\nShow the code\n# save model coefficients for a fitted model object from a workflow\n\nget_model <- function(x) {\n  pull_workflow_fit(x) %>% tidy()\n}\n\n# same as before with one exception\nlog_res_2 <- \n  log_wflow %>% \n  fit_resamples(\n    resamples = house_folds, \n    metrics = metric_set(\n      recall, precision, f_meas, \n      accuracy, kap,\n      roc_auc, sens, spec),\n    control = control_resamples(\n      save_pred = TRUE,\n      extract = get_model) # use extract and our new function\n    ) \n\n\n! Fold5: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: More than one set of outcomes were used when tuning. This should never\nhappen. Review how the outcome is specified in your model.\n\n\nShow the code\nlog_res_2$.extracts[[1]]\n\n\nNULL\n\n\nTo get the results use:\n\n\nShow the code\nlog_res_2$.extracts[[1]][[1]]\n\n\nNULL\n\n\nAll of the results can be flattened and collected using:\n\n\nShow the code\nall_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])\nfilter(all_coef, term == \"bedrooms\")\n\n\n# A tibble: 4 × 5\n  term     estimate std.error statistic p.value\n  <chr>       <dbl>     <dbl>     <dbl>   <dbl>\n1 bedrooms    0.106    0.0424      2.49 0.0129 \n2 bedrooms    0.106    0.0415      2.57 0.0103 \n3 bedrooms    0.117    0.0417      2.81 0.00502\n4 bedrooms    0.138    0.0422      3.28 0.00104\n\n\n\nPerformance metrics\n\n\nShow the code\nlog_res %>%  collect_metrics(summarize = TRUE)\n\n\n# A tibble: 8 × 6\n  .metric   .estimator  mean     n std_err .config             \n  <chr>     <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy  binary     0.820     5 0.00454 Preprocessor1_Model1\n2 f_meas    binary     0.816     5 0.00496 Preprocessor1_Model1\n3 kap       binary     0.640     5 0.00909 Preprocessor1_Model1\n4 precision binary     0.835     5 0.00404 Preprocessor1_Model1\n5 recall    binary     0.798     5 0.00660 Preprocessor1_Model1\n6 roc_auc   binary     0.899     5 0.00372 Preprocessor1_Model1\n7 sens      binary     0.798     5 0.00660 Preprocessor1_Model1\n8 spec      binary     0.842     5 0.00388 Preprocessor1_Model1\n\n\nShow performance for every single fold:\n\n\nShow the code\nlog_res %>%  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 40 × 5\n   id    .metric   .estimator .estimate .config             \n   <chr> <chr>     <chr>          <dbl> <chr>               \n 1 Fold1 recall    binary         0.788 Preprocessor1_Model1\n 2 Fold1 precision binary         0.834 Preprocessor1_Model1\n 3 Fold1 f_meas    binary         0.810 Preprocessor1_Model1\n 4 Fold1 accuracy  binary         0.816 Preprocessor1_Model1\n 5 Fold1 kap       binary         0.631 Preprocessor1_Model1\n 6 Fold1 sens      binary         0.788 Preprocessor1_Model1\n 7 Fold1 spec      binary         0.843 Preprocessor1_Model1\n 8 Fold1 roc_auc   binary         0.902 Preprocessor1_Model1\n 9 Fold2 recall    binary         0.778 Preprocessor1_Model1\n10 Fold2 precision binary         0.822 Preprocessor1_Model1\n# … with 30 more rows\n\n\n\n\nCollect predictions\nTo obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:\n\n\nShow the code\nlog_pred <- \n  log_res %>%\n  collect_predictions()\n\nlog_pred %>% \n  conf_mat(price_category, .pred_class) \n\n\n          Truth\nPrediction above below\n     above  4339   857\n     below  1098  4572\n\n\n\n\nShow the code\nlog_pred %>% \n  conf_mat(price_category, .pred_class) %>% \n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nROC Curve\n\n\nShow the code\nlog_pred %>% \n  group_by(id) %>% # id contains our folds\n  roc_curve(price_category, .pred_above) %>% \n  autoplot()\n\n\n\n\n\n\n\nShow the code\nlog_pred %>% \n  ggplot() +\n  geom_density(aes(x = .pred_above, \n                   fill = price_category), \n               alpha = 0.5)"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html",
    "href": "posts/Time-Series/TimeSeries.html",
    "title": "Time Series Project",
    "section": "",
    "text": "In this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\nHere is the link: https://www.zillow.com/research/data/"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#the-data",
    "href": "posts/Time-Series/TimeSeries.html#the-data",
    "title": "Time Series Project",
    "section": "The Data",
    "text": "The Data\n\n\nShow the code\nmetrofour <- read.csv(\"C:/Users/karol/Desktop/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ X2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ X2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,…)\n\n\nShow the code\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\n\n\nShow the code\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\n\n\nShow the code\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n\n\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3702562     Length:3702562    \n 1st Qu.: 17364   1st Qu.: 3506   Class :character   Class :character  \n Median : 31949   Median : 7193   Mode  :character   Mode  :character  \n Mean   : 51588   Mean   : 8231                                        \n 3rd Qu.: 46308   3rd Qu.:11702                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3702562     Length:3702562     Length:3702562     Length:3702562    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3702562     Min.   :  18032   Min.   :2000-01-31  \n Class :character   1st Qu.: 172756   1st Qu.:2005-09-30  \n Mode  :character   Median : 244796   Median :2011-06-15  \n                    Mean   : 321468   Mean   :2011-06-15  \n                    3rd Qu.: 368862   3rd Qu.:2017-02-28  \n                    Max.   :8241271   Max.   :2022-10-31  \n                    NA's   :1195958                       \n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\nShow the code\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n\n\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:106         Length:106        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:106         Length:106         Length:106         Length:106        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:106         Min.   :411049   Min.   :2014-01-31  \n Class :character   1st Qu.:505802   1st Qu.:2016-04-07  \n Mode  :character   Median :564917   Median :2018-06-15  \n                    Mean   :562387   Mean   :2018-06-15  \n                    3rd Qu.:584219   3rd Qu.:2020-08-23  \n                    Max.   :759661   Max.   :2022-10-31  \n\n\nAfter filtering the data, we don’t have any missing values\n\nCoerce to a tsibble with as_tsibble()\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\nShow the code\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#data-visualization",
    "href": "posts/Time-Series/TimeSeries.html#data-visualization",
    "title": "Time Series Project",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\nShow the code\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2012-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nShow the code\nplot_pdx_house \n\n\n\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\nShow the code\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nShow the code\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#determining-stationarity",
    "href": "posts/Time-Series/TimeSeries.html#determining-stationarity",
    "title": "Time Series Project",
    "section": "Determining Stationarity",
    "text": "Determining Stationarity\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\nShow the code\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n\n\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.95        0.01\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\nShow the code\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n\n\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary."
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#autocorrelation",
    "href": "posts/Time-Series/TimeSeries.html#autocorrelation",
    "title": "Time Series Project",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n\n\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThese are also clearly non-stationary, so we take a further first difference\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12) %>% difference(),\n               plot_type='partial', lag=36) +\n  labs(title = \"Double differenced\", y=\"\")\n\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 13 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph."
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#seasonal-arima-model",
    "href": "posts/Time-Series/TimeSeries.html#seasonal-arima-model",
    "title": "Time Series Project",
    "section": "Seasonal Arima Model",
    "text": "Seasonal Arima Model\n\n\nShow the code\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n\n\n\n\nShow the code\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n\n\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n\n\n\n\nShow the code\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.86   -189.  391.  392.  409.\n2 search        2.17   -190.  396.  398.  418.\n3 stepwise      2.25   -193.  399.  400.  418.\n4 arima210011   4.58   -205.  419.  419.  429.\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\nShow the code\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n\n\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5148  0.0355  1.0100  0.9999  -0.8229  0.1287\ns.e.  0.1145  0.1214  0.0582  0.0709   0.1477  0.1514\n\nsigma^2 estimated as 2.856:  log likelihood=-188.55\nAIC=391.09   AICc=392.41   BIC=408.82\n\n\n\n\nShow the code\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n\n\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    37.3     0.170\n\n\n\n\nShow the code\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model\\napplied to the Portland House Prices data\")\n\n\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`\n\n\n\n\n\nShow the code\n##Price in Thousands of Dollars\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast()\n\n\n# A fable: 24 x 5 [1M]\n# Key:     RegionName, .model [1]\n   RegionName .model                                  year_m…¹      Prices .mean\n   <chr>      <chr>                                      <mth>      <dist> <dbl>\n 1 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Nov   N(737, 3)  737.\n 2 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2022 Dec  N(737, 22)  737.\n 3 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jan  N(739, 76)  739.\n 4 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Feb N(742, 157)  742.\n 5 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Mar N(748, 257)  748.\n 6 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Apr N(753, 369)  753.\n 7 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 May N(758, 487)  758.\n 8 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jun N(761, 609)  761.\n 9 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Jul N(764, 734)  764.\n10 Portland   ARIMA(Prices ~ pdq(2, 1, 2) + PDQ(0, 1… 2023 Aug N(765, 860)  765.\n# … with 14 more rows, and abbreviated variable name ¹​year_month"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#ets",
    "href": "posts/Time-Series/TimeSeries.html#ets",
    "title": "Time Series Project",
    "section": "ETS",
    "text": "ETS\n\n\nShow the code\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n\n\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9987271 \n    phi   = 0.9108026 \n\n  Initial states:\n     l[0]     b[0]\n 407.9964 3.526883\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n650.8419 651.6904 666.8225 \n\n\nThe model selected is ETS(M,Ad,N)\n\n\nShow the code\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\nShow the code\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()\n\n\nPlot variable not specified, automatically selected `.vars = value`\n\n\n\n\n\n\n\nShow the code\nfit_ets%>%\n    gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)\n\n\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`\n\n\n\n\n\n\n\nShow the code\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.53  1.14 0.194 0.0286 0.0310\n2 Portland   ETS(Prices) Training  2.27  1.60 0.271 0.0403 0.0459\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE."
  }
]