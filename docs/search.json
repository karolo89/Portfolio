[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Dec 4, 2022\n\n\nKarol Orozco\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nKarol Orozco\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Employee Turnover/Analyzing Employee Turnover.html",
    "href": "posts/Employee Turnover/Analyzing Employee Turnover.html",
    "title": "Predicting Employee Attrition",
    "section": "",
    "text": "You work for the human capital department of a large corporation. The Board is worried about the relatively high turnover, and your team must look into ways to reduce the number of employees leaving the company. The team needs to understand better the situation, which employees are more likely to leave, and why. Once it is clear what variables impact employee churn, you can present your findings along with your ideas on how to attack the problem.\n\n\nCreate a report that covers the following:\n\nWhich designation has the highest employee turnover? Which one has the lowest?\nInvestigate which variables seem to be better predictors of employee departure.\nWhat recommendations would you make regarding ways to reduce employee turnover?"
  },
  {
    "objectID": "posts/Employee Turnover/Analyzing Employee Turnover.html#get-the-data",
    "href": "posts/Employee Turnover/Analyzing Employee Turnover.html#get-the-data",
    "title": "Predicting Employee Attrition",
    "section": "2. Get the Data",
    "text": "2. Get the Data\n\n\n  X     MMM.YY Emp_ID Age Gender City Education_Level Salary Dateofjoining\n1 1 2016-01-01      1  28   Male  C23          Master  57387    2015-12-24\n2 2 2016-02-01      1  28   Male  C23          Master  57387    2015-12-24\n3 3 2016-03-01      1  28   Male  C23          Master  57387    2015-12-24\n4 4 2017-11-01      2  31   Male   C7          Master  67016    2017-11-06\n5 5 2017-12-01      2  31   Male   C7          Master  67016    2017-11-06\n6 6 2016-12-01      4  43   Male  C13          Master  65603    2016-12-07\n  LastWorkingDate Joining.Designation Designation Total.Business.Value\n1            <NA>                   1           1              2381060\n2            <NA>                   1           1              -665480\n3      2016-03-11                   1           1                    0\n4            <NA>                   2           2                    0\n5            <NA>                   2           2                    0\n6            <NA>                   2           2                    0\n  Quarterly.Rating\n1                2\n2                2\n3                2\n4                1\n5                1\n6                1\n\n\n\nThe variables are defined as follow:\n\nMMMM-YY: Reporting Date (Monthly)\nEmp_ID: Unique id for employees\nAge: ge of the employee\nGender: Gender of the employee\nCity: City Code of the employee\nEducation_Level: Education level : Bachelor, Master or College\nSalary: Salary of the employee\nDateofjoining: Joining date for the employee\nLastWorkingDate: Last date of working for the employee\nJoining Designation: Designation of the employee at the time of joining\nDesignation: Designation of the employee at the time of reporting\nTotalBusinessValue: The total business value acquired by the employee in a month (negative business indicates cancellation/refund of sold insurance policies)\nQuarterly Rating: Quarterly rating of the employee: 1,2,3,4 (higher is better)"
  },
  {
    "objectID": "posts/Employee Turnover/Analyzing Employee Turnover.html#prepare-and-analyze-the-data",
    "href": "posts/Employee Turnover/Analyzing Employee Turnover.html#prepare-and-analyze-the-data",
    "title": "Predicting Employee Attrition",
    "section": "3. Prepare and Analyze the Data",
    "text": "3. Prepare and Analyze the Data\n\nThis dataset contains 19104 obs. of 13 variables. We see some duplicate Employees ID, and we should remove them\n\n\n\n\nNow we have 2,381 rows\n\n\n      X                 MMM.YY              Emp_ID               Age       \n Length:2381        Min.   :2016-01-01   Length:2381        Min.   :21.00  \n Class :character   1st Qu.:2016-01-01   Class :character   1st Qu.:29.00  \n Mode  :character   Median :2016-07-01   Mode  :character   Median :33.00  \n                    Mean   :2016-08-29                      Mean   :33.09  \n                    3rd Qu.:2017-05-01                      3rd Qu.:37.00  \n                    Max.   :2017-12-01                      Max.   :58.00  \n                                                                           \n    Gender         City           Education_Level     Salary      \n Female: 977   Length:2381        Bachelor:795    Min.   : 10747  \n Male  :1404   Class :character   College :784    1st Qu.: 39104  \n               Mode  :character   Master  :802    Median : 55276  \n                                                  Mean   : 59209  \n                                                  3rd Qu.: 75765  \n                                                  Max.   :188418  \n                                                                  \n Dateofjoining        LastWorkingDate      Joining.Designation Designation\n Min.   :2010-04-01   Min.   :2015-12-31   1:1026              1:751      \n 1st Qu.:2015-06-29   1st Qu.:2016-01-07   2: 815              2:866      \n Median :2016-07-21   Median :2016-01-19   3: 493              3:611      \n Mean   :2016-02-08   Mean   :2016-03-12   4:  36              4:132      \n 3rd Qu.:2017-05-02   3rd Qu.:2016-01-26   5:  11              5: 21      \n Max.   :2017-12-28   Max.   :2017-12-18                                  \n                      NA's   :2279                                        \n Total.Business.Value Quarterly.Rating\n Min.   :-1590270     1:1649          \n 1st Qu.:       0     2: 411          \n Median :       0     3: 216          \n Mean   :  291175     4: 105          \n 3rd Qu.:  250590                     \n Max.   :10398160                     \n                                      \n\n\n\nThe median age is 33 years old\nNow, we need to know that in order to predict if the employee will leave the company or not can be best predicted if we have last working day info available and hence although the missing percentage is high, it’s not because of Null values but the employees are not planning to leave the company and hence have not provided the info.Therefore we should not remove this feature from our data, instead we need to treat this as our target variable.\nSalary goes from 10747 to 188418 dollars\nMost of the employees have a Master degree\n\n\n\nShow the code\nempl_churn <- empl_turnover %>% mutate(Churn=  ifelse(is.na(LastWorkingDate), \"No\", \"Yes\"))\n\nempl_churn$Churn.Numeric <- as.numeric(as.factor(empl_churn$Churn))-1\n\nglimpse(empl_churn)\n\n\nRows: 2,381\nColumns: 16\n$ X                    <chr> \"1\", \"4\", \"6\", \"11\", \"14\", \"19\", \"22\", \"23\", \"29\"…\n$ MMM.YY               <date> 2016-01-01, 2017-11-01, 2016-12-01, 2016-01-01, …\n$ Emp_ID               <chr> \"1\", \"2\", \"4\", \"5\", \"6\", \"8\", \"11\", \"12\", \"13\", \"…\n$ Age                  <int> 28, 31, 43, 29, 31, 34, 28, 35, 29, 39, 30, 42, 2…\n$ Gender               <fct> Male, Male, Male, Male, Female, Male, Female, Mal…\n$ City                 <chr> \"C23\", \"C7\", \"C13\", \"C9\", \"C11\", \"C2\", \"C19\", \"C2…\n$ Education_Level      <fct> Master, Master, Master, College, Bachelor, Colleg…\n$ Salary               <int> 57387, 67016, 65603, 46368, 78728, 70656, 42172, …\n$ Dateofjoining        <date> 2015-12-24, 2017-11-06, 2016-12-07, 2016-01-09, …\n$ LastWorkingDate      <date> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Joining.Designation  <fct> 1, 2, 2, 1, 3, 3, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1…\n$ Designation          <fct> 1, 2, 2, 1, 3, 3, 1, 1, 4, 3, 2, 1, 1, 3, 1, 1, 2…\n$ Total.Business.Value <int> 2381060, 0, 0, 0, 0, 0, 0, 500000, 250000, 0, 346…\n$ Quarterly.Rating     <fct> 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 4, 2, 2…\n$ Churn                <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"…\n$ Churn.Numeric        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nTurnover\nNumber of employees that left the company:\nThe overall attrition rate in the company in average is 4.28%, which means that 102 employees have left the company during 2016-2017.``\n\n\nShow the code\ntable(empl_churn$Churn)\n\n\n\n  No  Yes \n2279  102 \n\n\n\n\nShow the code\nggplot(empl_churn,aes(x = Churn, group = Gender)) + \n    geom_bar(aes(y = ..prop.., fill = factor(..x..)), \n    stat=\"count\", color = \"black\", alpha = 0.7) +\n  \n    geom_text(aes(label = scales::percent(..prop..),\n    y= ..prop.. ), stat= \"count\", position = position_stack(vjust = 0.5), size = 4.5) +\n  \n    facet_grid(~Gender) +\n  \n    scale_y_continuous(labels = scales::percent)+\n\n    guides(fill = F)+\n    labs(\n    x = \"Left\", y = \"Percentage\",\n    title=\"Turnover Proportion by Gender\")+\n   scale_fill_manual(values=c(\"gray\", \"#2166AC\"))+\n    theme(text = element_text(size = 12), \n    plot.title = element_text(hjust = 0.5), \n    title = element_text(size = 12))+\n\n\n ggpubr::theme_pubclean()\n\n\n\n\n\nThere is a small difference in the proportion of women and men that left the company.\n\n\nShow the code\nempl_turnover_prop <- empl_churn %>% \n  tabyl(Designation, Churn) %>%\n  adorn_percentages(\"row\") \nkable(empl_turnover_prop,  caption = \"Designation vs Attrition\")\n\n\n\nDesignation vs Attrition\n\n\nDesignation\nNo\nYes\n\n\n\n\n1\n0.9227696\n0.0772304\n\n\n2\n0.9607390\n0.0392610\n\n\n3\n0.9852700\n0.0147300\n\n\n4\n1.0000000\n0.0000000\n\n\n5\n0.9523810\n0.0476190\n\n\n\n\n\n\nThe data shows that Destination 1 and 5 has the highest employee turnover rate, destination 4 and 3 has the lowest rate.\n\n\n\nAttrition vs Age\nThe majority of employees that left the organization are in their 30’s , and the proportion steadily declined.\n\n\nShow the code\n### Age vs Turnover\n\n\nempl_churn %>%\ngroup_by(Age, Churn)%>%\nggplot(aes(x = Age, fill = Churn)) +\ngeom_histogram(bins = 30L) +\n\nscale_fill_manual(values=c(\"gray\", \"#2166AC\"))+\n\n ggpubr::theme_pubclean()+\n\n\n  labs( x= \"Age\",\n        y= \"Number of Employees\",\n        title = \"Attrition vs Age\")+\n\ntheme(plot.title = element_text(size =12, face = \"bold\", hjust = 0.45),\nlegend.position=\"bottom\")"
  },
  {
    "objectID": "posts/Employee Turnover/Analyzing Employee Turnover.html#education-vs-churn",
    "href": "posts/Employee Turnover/Analyzing Employee Turnover.html#education-vs-churn",
    "title": "Predicting Employee Attrition",
    "section": "Education vs Churn",
    "text": "Education vs Churn\n\n\nShow the code\nresult <- compare_props(\n  empl_churn, \n  var1 = \"Education_Level\", \n  var2 = \"Churn\", \n  levs = \"No\", \n  comb = \"Bachelor:College\"\n)\nsummary(result, show = FALSE)\n\n\nPairwise proportion comparisons\nData      : empl_churn \nVariables : Education_Level, Churn \nLevel     : No in Churn \nConfidence: 0.95 \nAdjustment: None \n\n Education_Level  No Yes     p   n n_missing    sd    se    me\n        Bachelor 766  29 0.964 795         0 0.187 0.007 0.013\n         College 740  44 0.944 784         0 0.230 0.008 0.016\n          Master 773  29 0.964 802         0 0.187 0.007 0.013\n\n Null hyp.            Alt. hyp.                       diff p.value  \n Bachelor = College   Bachelor not equal to College   0.02 0.063   .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\nplot(result, plots = \"dodge\", custom = TRUE)+\n\nscale_fill_manual(values=c(\"gray\", \"#2166AC\"))+\n\n ggpubr::theme_pubclean()+\n\n\n  labs( x= \" \",\n        y= \"% of Churn\",\n        title = \"Attrition vs Education\")+\n\ntheme(plot.title = element_text(size =12, face = \"bold\", hjust = 0.45),\nlegend.position=\"bottom\")\n\n\n\n\n\n\nEmployees with College degree have the highest number of people living the company\n\n\n\nShow the code\nlibrary(corrplot)\nGGally::ggpairs(empl_churn[, unlist(lapply(empl_churn, is.numeric))])"
  },
  {
    "objectID": "posts/Employee Turnover/Analyzing Employee Turnover.html#the-model--binary-logistic-regression-for-churn-prediction",
    "href": "posts/Employee Turnover/Analyzing Employee Turnover.html#the-model--binary-logistic-regression-for-churn-prediction",
    "title": "Predicting Employee Attrition",
    "section": "4. The Model- Binary Logistic regression for churn prediction",
    "text": "4. The Model- Binary Logistic regression for churn prediction\n\n\nShow the code\nggplot(empl_churn) +\n aes(x = Salary, y = Churn, fill = Churn) +\n geom_boxplot() +\nscale_fill_manual(values=c(\"gray\", \"#2166AC\"))+\n ggpubr::theme_pubclean()"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html",
    "href": "posts/Time-Series/TimeSeries.html",
    "title": "Time Series Project",
    "section": "",
    "text": "In this project, I will perform Time series analysis using the Zillow Home Value Index (ZHVI) dataset: A smoothed, seasonally adjusted measure of the typical home value and market changes across Portland, OR, four bedroom houses. It reflects the typical value for homes in the 35th to 65th percentile range.\nHere is the link: https://www.zillow.com/research/data/"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#the-data",
    "href": "posts/Time-Series/TimeSeries.html#the-data",
    "title": "Time Series Project",
    "section": "The Data",
    "text": "The Data\n\n\nShow the code\nmetrofour <- read.csv(\"C:/Users/karol/Desktop/City_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID   : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank   : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName : chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType : chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName  : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State      : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro      : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName : chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ X2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ X2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ X2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\nWe have to make this dataset tidy. Tidy Data is a way of structuring data so that it can be easily understood by people and analyzed by machines.\nI need to remove the X at the beginning of the dates (X2000.01.31,X2000.02.29,…)\n\n\nShow the code\nnames(metrofour) <- sub(\"^X\", \"\", names(metrofour))\n\nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\n\n\nShow the code\nhouse_price <- metrofour %>% \n  pivot_longer(-c(RegionID, SizeRank, RegionName, RegionType, StateName, State, Metro, CountyName),\n    names_to = \"Monthly\",\n    values_to = \"Price\"\n  ) \nstr(metrofour[,c(1:11)])\n\n\n'data.frame':   13513 obs. of  11 variables:\n $ RegionID  : int  6181 12447 39051 17426 6915 40326 13271 18959 54296 38128 ...\n $ SizeRank  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ RegionName: chr  \"New York\" \"Los Angeles\" \"Houston\" \"Chicago\" ...\n $ RegionType: chr  \"city\" \"city\" \"city\" \"city\" ...\n $ StateName : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ State     : chr  \"NY\" \"CA\" \"TX\" \"IL\" ...\n $ Metro     : chr  \"New York-Newark-Jersey City, NY-NJ-PA\" \"Los Angeles-Long Beach-Anaheim, CA\" \"Houston-The Woodlands-Sugar Land, TX\" \"Chicago-Naperville-Elgin, IL-IN-WI\" ...\n $ CountyName: chr  \"Queens County\" \"Los Angeles County\" \"Harris County\" \"Cook County\" ...\n $ 2000.01.31: num  285211 302931 146651 181138 153807 ...\n $ 2000.02.29: num  287376 303230 146544 181616 154106 ...\n $ 2000.03.31: num  289205 304587 146194 182517 154357 ...\n\n\n\n\nShow the code\n#Converting the Date from factor to character\n\nhouse_clean <- house_price %>%\n            mutate(Monthly_parsed = as.Date(Monthly,\"%Y.%m.%d\"))\n\n\nhouse_clean[[\"Monthly\"]]<- as.character(house_clean$Monthly)\n\nhouse_price[[\"Monthly\"]]<- as.character(house_price $Monthly)\nsummary(house_clean)\n\n\n    RegionID         SizeRank      RegionName         RegionType       \n Min.   :  3300   Min.   :    0   Length:3702562     Length:3702562    \n 1st Qu.: 17364   1st Qu.: 3506   Class :character   Class :character  \n Median : 31949   Median : 7193   Mode  :character   Mode  :character  \n Mean   : 51588   Mean   : 8231                                        \n 3rd Qu.: 46308   3rd Qu.:11702                                        \n Max.   :827230   Max.   :28439                                        \n                                                                       \n  StateName            State              Metro            CountyName       \n Length:3702562     Length:3702562     Length:3702562     Length:3702562    \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price         Monthly_parsed      \n Length:3702562     Min.   :  18032   Min.   :2000-01-31  \n Class :character   1st Qu.: 172756   1st Qu.:2005-09-30  \n Mode  :character   Median : 244796   Median :2011-06-15  \n                    Mean   : 321468   Mean   :2011-06-15  \n                    3rd Qu.: 368862   3rd Qu.:2017-02-28  \n                    Max.   :8241271   Max.   :2022-10-31  \n                    NA's   :1195958                       \n\n\nWe see some missing values in the Price variable, but before I deal with those values, I will filter my data to the cities that I am interested the most\n\n\nShow the code\npdx_data <- house_clean %>%\n  dplyr:::filter(RegionID== 13373)  %>%\n  dplyr:::filter(Monthly_parsed >= \"2014-01-01\")\n\nsummary(pdx_data)\n\n\n    RegionID        SizeRank   RegionName         RegionType       \n Min.   :13373   Min.   :22   Length:106         Length:106        \n 1st Qu.:13373   1st Qu.:22   Class :character   Class :character  \n Median :13373   Median :22   Mode  :character   Mode  :character  \n Mean   :13373   Mean   :22                                        \n 3rd Qu.:13373   3rd Qu.:22                                        \n Max.   :13373   Max.   :22                                        \n  StateName            State              Metro            CountyName       \n Length:106         Length:106         Length:106         Length:106        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Monthly              Price        Monthly_parsed      \n Length:106         Min.   :411049   Min.   :2014-01-31  \n Class :character   1st Qu.:505802   1st Qu.:2016-04-07  \n Mode  :character   Median :564917   Median :2018-06-15  \n                    Mean   :562387   Mean   :2018-06-15  \n                    3rd Qu.:584219   3rd Qu.:2020-08-23  \n                    Max.   :759661   Max.   :2022-10-31  \n\n\nAfter filtering the data, we don’t have any missing values\n\nCoerce to a tsibble with as_tsibble()\nA time series can be recorded as a tsibble object in R. tsibble objects extend tidy data frames (tibble objects) by introducing temporal structure, and to do it, we need to declare key and index. In this case, the Monthly_parsed containing the data-time is the index and the RegionID is the key. Other columns can be considered as measured variables.\n\n\nShow the code\ntsb_pdx <- pdx_data %>%\n                   select(RegionName,RegionID, Monthly_parsed, Price)\n\ntsb_pref_pdx <-tsb_pdx%>%\n  as_tsibble(key= RegionName, index= Monthly_parsed)%>%\n                   index_by(year_month = ~ yearmonth(.))\n\ntsibble_pdx <-tsb_pref_pdx%>%\n  select(-RegionID)%>%\n  as_tsibble(key= RegionName, index= year_month)%>%\n  mutate(Prices = Price/1000)"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#data-visualization",
    "href": "posts/Time-Series/TimeSeries.html#data-visualization",
    "title": "Time Series Project",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo visualize the data, I could use the autoplot() command, but I rather to create my graph with ggplot.\n\n\nShow the code\nplot_pdx_house <- tsibble_pdx %>%\n  ggplot(aes(x= year_month, y= Prices)) +\n  geom_line(size=1, color= \"darkgreen\")+\n   \n    labs(y=\"Price in Thousands of Dollars \", \n       x= \" \",\n       title=\" Four Bedroom House Prices in Portland, OR, 2012-2022 \",\n       caption = \"data:https://www.zillow.com/research/data\")+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nShow the code\nplot_pdx_house \n\n\n\n\n\nData is non- stationary, we can see a trend-cycle component in the graph above.\n\n\nShow the code\ntsibble_pdx %>%\ngg_subseries(Price/1000)+\n  labs(y= \"Price in Thousands of Dollars\",\n       x= \"Year\")+theme_minimal()+\n  scale_y_continuous(labels=scales::dollar_format())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nShow the code\ntsibble_pdx%>%\ngg_season(Price/1000, labels = \"both\")+\n  labs(x= \"\",\n       y= \"Price in Thousands of Dollars \", \n       title=\"Portland's Seasonal Plot\")+\n  \n  scale_y_continuous(labels=scales::dollar_format())+\n  theme_minimal()"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#determining-stationarity",
    "href": "posts/Time-Series/TimeSeries.html#determining-stationarity",
    "title": "Time Series Project",
    "section": "Determining Stationarity",
    "text": "Determining Stationarity\nIn our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required. The test can be computed using the unitroot_kpss() function.\n\n\nShow the code\ntsibble_pdx%>%\n  features(Prices, unitroot_kpss)\n\n\n# A tibble: 1 × 3\n  RegionName kpss_stat kpss_pvalue\n  <chr>          <dbl>       <dbl>\n1 Portland        1.95        0.01\n\n\nThe p-value is reported as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic (1.946) is bigger than the 1% critical value, so the p-value is less than 0.01, indicating that the null hypothesis is rejected. That is, the data are not stationary.\n\n\nShow the code\ntsibble_pdx %>% \n  features(Prices ,unitroot_ndiffs)\n\n\n# A tibble: 1 × 2\n  RegionName ndiffs\n  <chr>       <int>\n1 Portland        1\n\n\nAs we saw from the KPSS tests above, one difference (d) is required to make the tsibble_pdx data stationary."
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#autocorrelation",
    "href": "posts/Time-Series/TimeSeries.html#autocorrelation",
    "title": "Time Series Project",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(Prices,\n                     plot_type='partial')+\n       labs(y=\"Thousands of Dollars \", \n       x= \" \")\n\n\n\n\n\nACF does not drop quickly to zero, moreover the value is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.\nPACF value r1 is almost 1. All other values ri,i >1 are small. This is a sign of a non stationary process that should be differenced in order to obtain a stationary series.\nThe data are clearly non-stationary, so we will first take a seasonal difference. The seasonally differenced data are shown below:\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12),\n               plot_type='partial', lag=36) +\n  labs(title=\"Seasonally differenced\", y=\"\")\n\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThese are also clearly non-stationary, so we take a further first difference\n\n\nShow the code\ntsibble_pdx %>%\n  gg_tsdisplay(difference(Prices, 12) %>% difference(),\n               plot_type='partial', lag=36) +\n  labs(title = \"Double differenced\", y=\"\")\n\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 13 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOur aim now is to find an appropriate ARIMA model based on the ACF and PACF shown in the Double Differenced graph."
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#seasonal-arima-model",
    "href": "posts/Time-Series/TimeSeries.html#seasonal-arima-model",
    "title": "Time Series Project",
    "section": "Seasonal Arima Model",
    "text": "Seasonal Arima Model\n\n\nShow the code\nall_fit <- tsibble_pdx%>%\n  model(\n    arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)),\n    arima210011 = ARIMA(Prices ~ pdq(2,1,0)+ PDQ(0,1,1)),\n    stepwise = ARIMA(Prices),\n    search = ARIMA(Prices,stepwise=FALSE))\n\n\n\n\nShow the code\nall_fit %>% pivot_longer(!RegionName,\n            names_to = \"Model name\", \n            values_to = \"Orders\")\n\n\n# A mable: 4 x 3\n# Key:     RegionName, Model name [4]\n  RegionName `Model name`                             Orders\n  <chr>      <chr>                                   <model>\n1 Portland   arima212012           <ARIMA(2,1,2)(0,1,2)[12]>\n2 Portland   arima210011           <ARIMA(2,1,0)(0,1,1)[12]>\n3 Portland   stepwise                <ARIMA(3,1,2) w/ drift>\n4 Portland   search       <ARIMA(2,1,3)(0,0,1)[12] w/ drift>\n\n\n\n\nShow the code\nglance(all_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n# A tibble: 4 × 6\n  .model      sigma2 log_lik   AIC  AICc   BIC\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 arima212012   2.86   -189.  391.  392.  409.\n2 search        2.17   -190.  396.  398.  418.\n3 stepwise      2.25   -193.  399.  400.  418.\n4 arima210011   4.58   -205.  419.  419.  429.\n\n\nOf these models, the best is the ARIMA(2,1,2)(0,1,2)[12]model (i.e., it has the smallest AICc value).\n\n\nShow the code\narima212012 <- tsibble_pdx %>%\n  model(arima212012 = ARIMA(Prices ~ pdq(2,1,2)+ PDQ(0,1,2)))%>%\n  report()\n\n\nSeries: Prices \nModel: ARIMA(2,1,2)(0,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1     ma2     sma1    sma2\n      0.5148  0.0355  1.0100  0.9999  -0.8229  0.1287\ns.e.  0.1145  0.1214  0.0582  0.0709   0.1477  0.1514\n\nsigma^2 estimated as 2.856:  log likelihood=-188.55\nAIC=391.09   AICc=392.41   BIC=408.82\n\n\n\n\nShow the code\nall_fit %>% select(arima212012) %>%\n  gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\naugment(all_fit) %>%\n  filter(.model=='arima212012') %>%\n  features(.innov, ljung_box, lag = 36, dof = 6)\n\n\n# A tibble: 1 × 4\n  RegionName .model      lb_stat lb_pvalue\n  <chr>      <chr>         <dbl>     <dbl>\n1 Portland   arima212012    37.3     0.170\n\n\n\n\nShow the code\ntsibble_pdx %>%\n  model(ARIMA(Prices ~ pdq(2,1,2) + PDQ(0,1,2))) %>%\n  forecast() %>%\n  autoplot(tsibble_pdx) +\n  labs(y=\" Thousands of $US \",\n       x =\" \",\n       title=\"Forecast from the ARIMA(2,1,2)(0,1,2)[12] model\\napplied to the Portland House Prices data\")\n\n\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`"
  },
  {
    "objectID": "posts/Time-Series/TimeSeries.html#ets",
    "href": "posts/Time-Series/TimeSeries.html#ets",
    "title": "Time Series Project",
    "section": "ETS",
    "text": "ETS\n\n\nShow the code\nfit_ets <- tsibble_pdx %>%\n  model(ETS(Prices))\nreport(fit_ets)\n\n\nSeries: Prices \nModel: ETS(M,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.9987271 \n    phi   = 0.9108026 \n\n  Initial states:\n     l[0]     b[0]\n 407.9964 3.526883\n\n  sigma^2:  0\n\n     AIC     AICc      BIC \n650.8419 651.6904 666.8225 \n\n\nThe model selected is ETS(M,Ad,N)\n\n\nShow the code\ncomponents(fit_ets) %>%\n  autoplot() +\n  labs(title = \"ETS(M,Ad,N) components\")\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nBecause this model has multiplicative errors, the innovation residuals are not equivalent to the regular residuals.\n\n\nShow the code\nfit_ets %>%\n    augment() %>%\n    select(.innov, .resid) %>%\n    pivot_longer(c(.innov, .resid)) %>%\n    autoplot()\n\n\nPlot variable not specified, automatically selected `.vars = value`\n\n\n\n\n\n\n\nShow the code\nfit_ets%>%\n    gg_tsresiduals()\n\n\n\n\n\n\n\nShow the code\nfit_ets %>%\n  forecast(h = 24) %>%\n  autoplot(tsibble_pdx)\n\n\n`mutate_if()` ignored the following grouping variables:\n• Column `year_month`\n\n\n\n\n\n\n\nShow the code\nbind_rows(\n    arima212012 %>% accuracy(),\n    fit_ets %>% accuracy()) %>%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 2 × 8\n  RegionName .model      .type     RMSE   MAE  MAPE   MASE  RMSSE\n  <chr>      <chr>       <chr>    <dbl> <dbl> <dbl>  <dbl>  <dbl>\n1 Portland   arima212012 Training  1.53  1.14 0.194 0.0286 0.0310\n2 Portland   ETS(Prices) Training  2.27  1.60 0.271 0.0403 0.0459\n\n\nIn this case the ARIMA model seems to be more accurate model based on the test set RMSE, MAPE and MASE."
  }
]